{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_CHT_edges_table(database_name, ParseDate):\n",
    "    conn = sqlite3.connect(database_name)\n",
    "    cur = conn.cursor()\n",
    "    myresult = cur.execute(\"SELECT News1,News2 FROM CHTconnection WHERE ParseDate==?\", (ParseDate,))\n",
    "    result = []\n",
    "    for row in myresult:\n",
    "        result.append(row)\n",
    "    conn.close()        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_CHT_nodes_table(database_name, NewsIndex):\n",
    "    conn = sqlite3.connect(database_name)\n",
    "    cur = conn.cursor()\n",
    "    myresult = cur.execute(\"SELECT NewsIndex,NewsTitle,NewsURL FROM CHTnews WHERE NewsIndex==?\", (NewsIndex,))\n",
    "    result = []\n",
    "    for row in myresult:\n",
    "        result.append(row)\n",
    "    conn.close()    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = '/Users/gtingyou/Documents/NewsNetwork/Recommendation System/IsorankBased/DailyNews/NewsNetwork_ch.db'\n",
    "DATE = '2020-04-06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓每日大網路圖 做network embedding\n",
    "CHTconnection = select_CHT_edges_table(DB_PATH, DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12672"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CHTconnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Output_CHTconnection_To_Edgelist_File(CHTconnection):\n",
    "    df_edgelist = pd.DataFrame(CHTconnection)\n",
    "    df_edgelist.to_csv('./GraphEmbedding/data/news/news_%s.edgelist' %(DATE),sep=' ',index=None,header=None)\n",
    "\n",
    "Output_CHTconnection_To_Edgelist_File(CHTconnection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### node2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Node2Vec_Network_Embedding():\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    from GraphEmbedding.ge.classify import read_node_label,Classifier\n",
    "    from GraphEmbedding.ge import Node2Vec\n",
    "    \n",
    "    G = nx.read_edgelist('./GraphEmbedding/data/news/news_%s.edgelist' %(DATE),\n",
    "                        create_using = nx.DiGraph(), nodetype = None, data = [('weight', int)])#read graph\n",
    "\n",
    "    model = Node2Vec(G, walk_length = 10, num_walks = 80,p = 0.25, q = 4, workers = 1)#init model\n",
    "    model.train(window_size = 5, iter = 3)# train model\n",
    "    embeddings = model.get_embeddings()# get embedding vectors\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess transition probs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning embedding vectors...\n",
      "Learning embedding vectors done!\n"
     ]
    }
   ],
   "source": [
    "node2vec_embeddings = Node2Vec_Network_Embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = node2vec_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Output_Embeddings_To_File(embeddings):\n",
    "    import pandas as pd\n",
    "    df_embeddings = pd.DataFrame(embeddings).T\n",
    "    df_embeddings= df_embeddings.sort_index()\n",
    "\n",
    "    df_embeddings.to_csv('./GraphEmbedding/emb/news_%s.emb' %(DATE),sep=' ',index=True,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_Embeddings_To_File(node2vec_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.獲取使用者瀏覽紀錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_USER_nodes_table(database_name, ParseDate):\n",
    "    conn = sqlite3.connect(database_name)\n",
    "    cur = conn.cursor()\n",
    "    myresult = cur.execute(\"SELECT NewsIndex,NewsTitle,NewsURL FROM NODES WHERE ParseDate==?\", (ParseDate,))\n",
    "    result = []\n",
    "    for row in myresult:\n",
    "        result.append(row)\n",
    "    conn.close()    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "userlogs = select_USER_nodes_table('/Users/gtingyou/Documents/NewsNetwork/Recommendation System/IsorankBased/DailyUserLogs/user1.db',\n",
    "                                    '2020-04-03')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.找出使用者瀏覽紀錄在大網路圖中的NewsIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Userlogs_NewsIndex_In_BigNetwork(userlogs):\n",
    "    def clean_CHT_url(url):\n",
    "        if '?chdtv' in url:\n",
    "            url = url.replace('?chdtv', '')\n",
    "        if '?ctrack' in url:\n",
    "            url = url[:url.find('?ctrack')]\n",
    "        return url\n",
    "    \n",
    "    def get_CHT_big_network_index(database_name, NewsURL):\n",
    "        conn = sqlite3.connect(database_name)\n",
    "        cur = conn.cursor()\n",
    "        myresult = cur.execute(\"SELECT NewsIndex,NewsTitle,NewsURL FROM CHTnews WHERE NewsURL like ?\", ('%'+NewsURL+'%',))\n",
    "        result = []\n",
    "        for row in myresult:\n",
    "            result.append(row)\n",
    "        conn.close()    \n",
    "        return result\n",
    "    \n",
    "    userlogs_NewsIndex = []\n",
    "    for log in user_logs:\n",
    "        if log[2]=='https://www.chinatimes.com/':\n",
    "            continue\n",
    "\n",
    "        result = get_CHT_big_network_index(DB_PATH, log[2])\n",
    "        if result!=[]:\n",
    "            userlogs_NewsIndex.append(result[0])\n",
    "        else: # 使用者瀏覽紀錄的網址，不在大網路圖中，所以找不到對應的 NewsIndex\n",
    "            print(log[1],clean_CHT_url(log[2]))\n",
    "            \n",
    "    return userlogs_NewsIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "疫情惡化 全球確診將破百萬 - 國際大事 https://www.chinatimes.com/newspapers/20200403000278-260119\n",
      "桃園2同住居家檢疫者 陳屍家中 - 社會 https://www.chinatimes.com/realtimenews/20200403003492-260402\n",
      "遊客群聚一窩蜂 社交距離破功 - 焦點要聞 https://www.chinatimes.com/newspapers/20200403000298-260114\n",
      "紐約確診8.4萬人 比陸全國還多 - 焦點要聞 https://www.chinatimes.com/newspapers/20200403000314-260119\n",
      " https://www.chinatimes.com/newspapers/2601\n",
      " https://www.chinatimes.com/newspapers/2601?page=2\n",
      " https://www.chinatimes.com/newspapers/2601?page=2&chdtv\n",
      "港防疫升級 酒吧今起關閉半個月 - 焦點要聞 https://www.chinatimes.com/newspapers/20200403000337-260108\n",
      "疫情惡化 美不再禁止陸製KN95口罩進口 - 國際 https://www.chinatimes.com/realtimenews/20200403003589-260408\n",
      "封國前兆？美國務院發推文：美國人快回來！ - 國際 https://www.chinatimes.com/realtimenews/20200403003659-260408\n",
      "封國前兆？美國務院發推文：美國人快回來！ - 國際 https://www.chinatimes.com/realtimenews/20200403003659-260408\n",
      "美Q2經濟慘到爆 衰幅恐達38％ - 財經 https://www.chinatimes.com/realtimenews/20200403003847-260410\n",
      "美Q2經濟慘到爆 衰幅恐達38％ - 財經 https://www.chinatimes.com/realtimenews/20200403003847-260410\n",
      "\n",
      "NewsIndex in user browsing history:\n",
      "--------------------------------------------------\n",
      "('CHT_20200406_521', '又現感染源不明病例 社區保全確診 憂社區感染破口 - 生活新聞', 'https://www.chinatimes.com/newspapers/20200403000283-260114')\n",
      "--------------------------------------------------\n",
      "('CHT_20200406_716', '防社區傳播 醫籲擴大採檢 - 生活新聞', 'https://www.chinatimes.com/newspapers/20200403000286-260114')\n",
      "--------------------------------------------------\n",
      "('CHT_20200406_138', '疫情嚴峻 影院苦等政府按暫停鍵 - 生活新聞', 'https://www.chinatimes.com/newspapers/20200403000305-260114')\n",
      "--------------------------------------------------\n",
      "('CHT_20200406_1364', '連假遷徙潮 蘇花改雪隧塞爆 - 生活新聞', 'https://www.chinatimes.com/newspapers/20200403000309-260114')\n",
      "--------------------------------------------------\n",
      "('CHT_20200406_3170', '青少年染疫數日死 世衛敲警鐘 - 國際大事', 'https://www.chinatimes.com/newspapers/20200403000319-260119')\n",
      "--------------------------------------------------\n",
      "('CHT_20200331_73', '陸出大招防控無症狀感染者 專家支持 - 兩岸', 'https://www.chinatimes.com/realtimenews/20200331001054-260409')\n",
      "--------------------------------------------------\n",
      "('CHT_20200331_9', '誰傳染給師大生？名醫示警：該堤防的是這類人 - 生活', 'https://www.chinatimes.com/realtimenews/20200331004467-260405')\n",
      "--------------------------------------------------\n",
      "('CHT_20200406_1223', '美CDC主任：無症狀感染者多達25％ 疫情或持續2年 - 兩岸', 'https://www.chinatimes.com/realtimenews/20200402004406-260409')\n",
      "--------------------------------------------------\n",
      "('CHT_20200401_146', '美軍疫情擴散 危機重重 - 論壇廣場', 'https://www.chinatimes.com/newspapers/20200331000218-260310')\n",
      "--------------------------------------------------\n",
      "('CHT_20200406_647', '美股噴400點是死貓跳！專家驚洩下波大海嘯 - 財經', 'https://www.chinatimes.com/realtimenews/20200403001633-260410')\n",
      "--------------------------------------------------\n",
      "('CHT_20200406_2239', '台股死撐…真的殺不下去？專家曝4點突破盲腸 - 財經', 'https://www.chinatimes.com/realtimenews/20200403000025-260410')\n"
     ]
    }
   ],
   "source": [
    "userlogs_NewsIndex = Get_Userlogs_NewsIndex_In_BigNetwork(userlogs)\n",
    "\n",
    "print('\\nNewsIndex in user browsing history:')\n",
    "for i in userlogs_NewsIndex: \n",
    "    print('-'*50)\n",
    "    print(select_CHT_nodes_table(DB_PATH, i[0])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.計算推薦新聞文檔相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Open_stopWords_From_File(path):\n",
    "    stopWords=[]\n",
    "    # 讀入停用詞檔\n",
    "    with open('stopWords.txt', 'r', encoding='UTF-8') as file:\n",
    "        for data in file.readlines():\n",
    "            data = data.strip()\n",
    "            stopWords.append(data)\n",
    "    return stopWords\n",
    "\n",
    "stopWords = Open_stopWords_From_File(path='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_News_Document_Similarity(emb_dict):\n",
    "    '''\n",
    "    1. get NewsDetail(include NewsIndex,NewsContext,NewsTitle) from database ----> time consuming\n",
    "    2. construct cosine similarity matix of context and title respectively\n",
    "    3. compare 0:context and 1:title doc sim score, return the bigger one\n",
    "    return the final cosine similarity matrix of all the News in Big network\n",
    "    '''\n",
    "    ### 1. get NewsDetail\n",
    "    def select_CHT_nodes_title_context(database_name, NewsIndex):\n",
    "        conn = sqlite3.connect(database_name)\n",
    "        cur = conn.cursor()\n",
    "        myresult = cur.execute(\"SELECT NewsIndex,NewsContext,NewsTitle FROM CHTnews WHERE NewsIndex==?\", (NewsIndex,))\n",
    "        result = []\n",
    "        for row in myresult:\n",
    "            result.append(row)\n",
    "        conn.close()    \n",
    "        return result\n",
    "    \n",
    "    CS_index_list = sorted(list(emb_dict.keys()))\n",
    "    News = []\n",
    "    for index in tqdm( CS_index_list ):\n",
    "        News.append(list(select_CHT_nodes_title_context(DB_PATH, index)[0]))\n",
    "    print('End loading NewsTitle and NewsContext from database ---> ', time.strftime(\"%H:%M:%S\", time.localtime()) )\n",
    "    \n",
    "    ### construct cs matrix\n",
    "    def Construct_CS_Matrix(context_or_title, News): # 算 context_or_title 的詞頻矩陣 ---> 1: context , 2: title\n",
    "        corpus = []\n",
    "        for i, k in enumerate(News):\n",
    "            # 結巴中文斷詞\n",
    "            segments = jieba.cut(k[context_or_title], cut_all=False)\n",
    "            # 移除停用詞及跳行符號\n",
    "            remainderWords = list(filter(lambda a: a not in stopWords and a != '\\n', segments))\n",
    "            corpus.append(' '.join(remainderWords))\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "        cs = cosine_similarity(X)\n",
    "        return cs    \n",
    "    \n",
    "    cs1 = Construct_CS_Matrix(1, News)\n",
    "    print('End constructing cosine similarity matrix for NewsContext ---> ', time.strftime(\"%H:%M:%S\", time.localtime()) )\n",
    "    cs2 = Construct_CS_Matrix(2, News)\n",
    "    print('End constructing cosine similarity matrix for NewsTitle ---> ', time.strftime(\"%H:%M:%S\", time.localtime()) )\n",
    "    \n",
    "    ### compare scores\n",
    "    compared_cs = np.zeros([len(cs1),len(cs1)])\n",
    "    for i in range(len(cs1)):\n",
    "        for j in range(len(cs1)):\n",
    "            if cs1[i][j]>=cs2[i][j]:\n",
    "                compared_cs[i][j] = cs1[i][j]\n",
    "            else:\n",
    "                compared_cs[i][j] = cs2[i][j]\n",
    "    return CS_index_list, compared_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5229/5229 [01:24<00:00, 62.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End loading NewsTitle and NewsContext from database --->  15:36:30\n",
      "End constructing cosine similarity matrix for NewsContext --->  15:37:13\n"
     ]
    }
   ],
   "source": [
    "document_similarity_matrix_index, document_similarity_matrix = Calculate_News_Document_Similarity(emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similar_Document_Content_News(NewsID,document_similarity_matrix,document_similarity_matrix_index):\n",
    "    row_num = document_similarity_matrix_index.index(NewsID)\n",
    "#     print(row_num)\n",
    "    doc_similarity_scores = [[document_similarity_matrix_index[i], \n",
    "                              document_similarity_matrix[row_num][i]] for i in range(len(document_similarity_matrix)) if i!=row_num] #排除自己\n",
    "    return sorted(doc_similarity_scores, key=lambda x:x[0], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_similarity_scores = Similar_Document_Content_News('CHT_20200406_2366',\n",
    "#                                                       document_similarity_matrix,document_similarity_matrix_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.利用node2vec embedding結果，計算topology相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similar_Topological_News(NewsID, emb_dict):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    v1 = emb_dict[NewsID] # News1 embedding\n",
    "    topology_similarity_scores = []\n",
    "    for i in emb_dict:\n",
    "        v2 = emb_dict[i] # News2 embedding\n",
    "        if list(v1)!=list(v2): \n",
    "            s = cosine_similarity( np.array(v1).reshape(1,128), np.array(v2).reshape(1,128) )\n",
    "            topology_similarity_scores.append([i, s[0][0]])\n",
    "    \n",
    "    return sorted(topology_similarity_scores, key=lambda x:x[0], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topology_similarity_scores = Similar_Topological_News(NewsID, emb_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.根據 *alpha x 文檔相似度+(1-alpha) x topology相似度* 推薦新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_Document_and_Topology_Score(doc_similarity_scores, topology_similarity_scores, standardize):\n",
    "    alpha = 0.5\n",
    "\n",
    "    if standardize==True: ### 對 document score和 topology score做標準化\n",
    "        def Standardization(data):\n",
    "            mu = np.mean(data, axis=0)\n",
    "            sigma = np.std(data, axis=0)\n",
    "            print('mean=%f, sigma=%f' %(mu, sigma))\n",
    "            return (data - mu) / sigma\n",
    "        print('Standardize document sim score:')\n",
    "        standardized_doc_score = Standardization(np.array([k[1] for k in doc_similarity_scores],dtype = 'float'))\n",
    "        doc_similarity_scores = [[doc_similarity_scores[i][0],\n",
    "                                  standardized_doc_score[i]] for i in range(len(doc_similarity_scores))]\n",
    "        print('Standardize topology sim score:')\n",
    "        standardized_topology_score = Standardization([k[1] for k in topology_similarity_scores])\n",
    "        topology_similarity_scores = [[topology_similarity_scores[i][0],\n",
    "                                       standardized_topology_score[i]] for i in range(len(topology_similarity_scores))]\n",
    "\n",
    "    ### 按照index編號順序排列\n",
    "    doc_similarity_scores = sorted(doc_similarity_scores, key=lambda x:x[0], reverse=False)\n",
    "    topology_similarity_scores = sorted(topology_similarity_scores, key=lambda x:x[0], reverse=False)\n",
    "    ### 依公式 alpha x 文檔相似度+(1-alpha) 計算合併的成績\n",
    "    merged_similarity_scores = []\n",
    "    for i in range(len(doc_similarity_scores)):\n",
    "        if doc_similarity_scores[i][0]==topology_similarity_scores[i][0]: # 檢查index編號是否相同\n",
    "            merged_similarity_scores.append([doc_similarity_scores[i][0], \n",
    "                                             alpha*float(doc_similarity_scores[i][1])+(1-alpha)*topology_similarity_scores[i][1] ])\n",
    "    \n",
    "    merged_similarity_score_dict = {merged_similarity_scores[i][0]:[merged_similarity_scores[i][1],\n",
    "                                                                    doc_similarity_scores[i][1],\n",
    "                                                                    topology_similarity_scores[i][1]] for i in range(len(merged_similarity_scores) ) }\n",
    "    \n",
    "    return merged_similarity_scores, merged_similarity_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_similarity_scores, merged_similarity_score_dict = Merge_Document_and_Topology_Score(doc_similarity_scores,\n",
    "#                                                                                           topology_similarity_scores,\n",
    "#                                                                                           True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recommend_News(NewsID, num_of_recommend_news):\n",
    "    print('User browsing history News:\\n', select_CHT_nodes_table(DB_PATH, NewsID)[0])\n",
    "    \n",
    "    doc_similarity_scores = Similar_Document_Content_News(NewsID,\n",
    "                                                          document_similarity_matrix,\n",
    "                                                          document_similarity_matrix_index)\n",
    "    topology_similarity_scores = Similar_Topological_News(NewsID, emb_dict)\n",
    "    merged_similarity_scores, merged_similarity_score_dict = Merge_Document_and_Topology_Score(doc_similarity_scores,\n",
    "                                                                                              topology_similarity_scores,\n",
    "                                                                                              False)\n",
    "    print('\\nRecommed News:')\n",
    "    for i, k in enumerate(sorted(merged_similarity_scores, key=lambda x:x[1], reverse=True)):\n",
    "        if i>num_of_recommend_news:\n",
    "            break\n",
    "        print('-'*50)\n",
    "        s = merged_similarity_score_dict[k[0]]\n",
    "        print('total_sim_score=%s, doc_sim_score=%s, topology_sim_score=%s' %(s[0], s[1], s[2]))\n",
    "        print(select_CHT_nodes_table(DB_PATH, k[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User browsing history News:\n",
      " ('CHT_20200406_1223', '美CDC主任：無症狀感染者多達25％ 疫情或持續2年 - 兩岸', 'https://www.chinatimes.com/realtimenews/20200402004406-260409')\n",
      "\n",
      "Recommed News:\n",
      "--------------------------------------------------\n",
      "total_sim_score=0.7085087815153607, doc_sim_score=0.4321163136219994, topology_sim_score=0.98490125\n",
      "('CHT_20200406_1259', '鍾南山警示：兩類無症狀感染者要特別注意 - 兩岸', 'https://www.chinatimes.com/realtimenews/20200403003026-260409')\n",
      "--------------------------------------------------\n",
      "total_sim_score=0.6959818330409381, doc_sim_score=0.40901852190539295, topology_sim_score=0.98294514\n",
      "('CHT_20200331_657', '檢測增加 隔離不力 防疫慢半拍！3大原因 美新冠病例飆至全球首位 - 國際大事', 'https://www.chinatimes.com/newspapers/20200328000492-260119')\n",
      "--------------------------------------------------\n",
      "total_sim_score=0.6929542085270844, doc_sim_score=0.40296911413287373, topology_sim_score=0.9829393\n",
      "('CHT_20200401_330', '隱形的危險 聚焦無症狀感染者 - 焦點新聞', 'https://www.chinatimes.com/newspapers/20200401000180-260301')\n",
      "--------------------------------------------------\n",
      "total_sim_score=0.6900945487097905, doc_sim_score=0.39921656896193947, topology_sim_score=0.9809725\n",
      "('CHT_20200328_351', '美國為何成疫情震央 - 時論廣場', 'https://www.chinatimes.com/newspapers/20200327000805-260109')\n",
      "--------------------------------------------------\n",
      "total_sim_score=0.6884630899524332, doc_sim_score=0.39156013347618596, topology_sim_score=0.98536605\n",
      "('CHT_20200406_1036', '美國1億5千萬人恐遭感染 傳染病專家坦承：我們必須承認失敗！ - 國際', 'https://www.chinatimes.com/realtimenews/20200314003292-260408')\n",
      "--------------------------------------------------\n",
      "total_sim_score=0.6881770125248585, doc_sim_score=0.3925329906182595, topology_sim_score=0.98382103\n",
      "('CHT_20200406_1029', '川普新令 擬限制醫療產品依賴中國 - 全球財經', 'https://www.chinatimes.com/newspapers/20200313000323-260203')\n",
      "--------------------------------------------------\n",
      "total_sim_score=0.687850416539304, doc_sim_score=0.4042332189047189, topology_sim_score=0.9714676\n",
      "('CHT_20200406_3016', '新冠病毒源自哪 專家仍無法確認 - 焦點新聞', 'https://www.chinatimes.com/newspapers/20200311000179-260301')\n",
      "--------------------------------------------------\n",
      "total_sim_score=0.6814921217165032, doc_sim_score=0.38037461380196386, topology_sim_score=0.9826096\n",
      "('CHT_20200328_283', '新冠疫情如噩夢場景 比爾蓋茲喊話美學陸嚴控 - 兩岸', 'https://www.chinatimes.com/realtimenews/20200327004500-260409')\n",
      "--------------------------------------------------\n",
      "total_sim_score=0.6791608676400278, doc_sim_score=0.3909280985765643, topology_sim_score=0.96739364\n",
      "('CHT_20200406_2484', '美CDC建議戴口罩 川普：很棒的政策，但我不戴', 'https://www.chinatimes.com/realtimenews/20200404001530-260408')\n",
      "--------------------------------------------------\n",
      "total_sim_score=0.6747426036582442, doc_sim_score=0.3715963847609466, topology_sim_score=0.9778888\n",
      "('CHT_20200406_1212', '最慘新冠疫情預估：2億美國人感染 170萬人病死 - 國際', 'https://www.chinatimes.com/realtimenews/20200315001636-260408')\n",
      "--------------------------------------------------\n",
      "total_sim_score=0.6742073342310092, doc_sim_score=0.3644985050174994, topology_sim_score=0.98391616\n",
      "('CHT_20200327_285', '這場與病毒的戰爭 - 時論廣場', 'https://www.chinatimes.com/newspapers/20200326000784-260109')\n"
     ]
    }
   ],
   "source": [
    "Recommend_News('CHT_20200406_1223', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 統計score區間次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardization(data):\n",
    "    mu = np.mean(data, axis=0)\n",
    "    sigma = np.std(data, axis=0)\n",
    "    print('mean=%f, sigma=%f' %(mu, sigma))\n",
    "    return (data - mu) / sigma\n",
    "\n",
    "def Normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.001, 0.1]    4292\n",
       "(0.1, 0.2]        865\n",
       "(0.2, 0.3]         52\n",
       "(0.3, 0.4]         11\n",
       "(0.4, 0.5]          4\n",
       "(0.5, 0.6]          2\n",
       "(0.6, 0.7]          1\n",
       "(0.7, 0.8]          1\n",
       "(0.8, 0.9]          0\n",
       "(0.9, 1.0]          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([k[1] for k in doc_similarity_scores]).value_counts(bins=[0.1*i for i in range(11)], sort=False)#.plot(kind='bar', ylim=(0,5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.057, 0.372]     4005\n",
       "(0.372, 1.785]      1003\n",
       "(1.785, 3.198]       154\n",
       "(3.198, 4.611]        46\n",
       "(4.611, 6.024]        10\n",
       "(6.024, 7.437]         3\n",
       "(7.437, 8.85]          3\n",
       "(8.85, 10.263]         1\n",
       "(10.263, 11.676]       1\n",
       "(11.676, 13.089]       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 原始doc_similarity_score分數偏低（左尾），標準化 doc_similarity_score\n",
    "pd.Series(Standardization([k[1] for k in doc_similarity_scores])).value_counts(bins=10,sort=False)#.plot(kind='bar', ylim=(0,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.001, 0.1]       0\n",
       "(0.1, 0.2]          1\n",
       "(0.2, 0.3]          0\n",
       "(0.3, 0.4]          0\n",
       "(0.4, 0.5]          1\n",
       "(0.5, 0.6]          0\n",
       "(0.6, 0.7]          2\n",
       "(0.7, 0.8]          3\n",
       "(0.8, 0.9]         67\n",
       "(0.9, 1.0]       5153\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([k[1] for k in topology_similarity_scores]).value_counts(bins=[0.1*i for i in range(11)], sort=False)#.plot(kind='bar', ylim=(0,5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=0.983300, sigma=0.029242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.001, 0.1]     371\n",
       "(0.1, 0.2]        601\n",
       "(0.2, 0.3]       1041\n",
       "(0.3, 0.4]       1963\n",
       "(0.4, 0.5]        106\n",
       "(0.5, 0.6]          0\n",
       "(0.6, 0.7]          0\n",
       "(0.7, 0.8]          0\n",
       "(0.8, 0.9]          0\n",
       "(0.9, 1.0]          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 原始topology_similarity_score分數偏高（右尾），標準化 topology_similarity_score\n",
    "pd.Series(Standardization([k[1] for k in topology_similarity_scores])).value_counts(bins=[0.1*i for i in range(11)],sort=False)#.plot(kind='bar', ylim=(0,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Construct_CS_Matrix(context_or_title, News):\n",
    "    corpus = []\n",
    "    for i, k in enumerate(News):\n",
    "        # 結巴中文斷詞\n",
    "        segments = jieba.cut(k[context_or_title], cut_all=False)\n",
    "        # 移除停用詞及跳行符號\n",
    "        remainderWords = list(filter(lambda a: a not in stopWords and a != '\\n', segments))\n",
    "        corpus.append(' '.join(remainderWords))\n",
    "        \n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    cs = cosine_similarity(X)\n",
    "#     print(cs.shape)\n",
    "    \n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_News_Document_Similarity(NewsIndex1,NewsIndex2,emb_dict):\n",
    "    def select_CHT_nodes_title_context(database_name, NewsIndex):\n",
    "        conn = sqlite3.connect(database_name)\n",
    "        cur = conn.cursor()\n",
    "        myresult = cur.execute(\"SELECT NewsIndex,NewsContext,NewsTitle FROM CHTnews WHERE NewsIndex==?\", (NewsIndex,))\n",
    "        result = []\n",
    "        for row in myresult:\n",
    "            result.append(row)\n",
    "        conn.close()    \n",
    "        return result\n",
    "    \n",
    "    News = []\n",
    "#     for index in tqdm( sorted(list(emb_dict.keys())) ):\n",
    "#         News.append(list(select_CHT_nodes_title_context(DB_PATH, index)[0]))\n",
    "    News.append(list(select_CHT_nodes_title_context(DB_PATH, NewsIndex1)[0]))\n",
    "    News.append(list(select_CHT_nodes_title_context(DB_PATH, NewsIndex2)[0]))\n",
    "#     print('End loading NewsTitle and NewsContext from database ---> ', time.strftime(\"%H:%M:%S\", time.localtime()) )\n",
    "        \n",
    "    # 算 context_or_title 的詞頻矩陣 ---> 1: context , 2: title\n",
    "    cs1 = Construct_CS_Matrix(1, News)\n",
    "#     print('End constructing cosine similarity matrix for NewsTitle ---> ', time.strftime(\"%H:%M:%S\", time.localtime()) )\n",
    "    cs2 = Construct_CS_Matrix(2, News)\n",
    "#     print('End constructing cosine similarity matrix for NewsContext ---> ', time.strftime(\"%H:%M:%S\", time.localtime()) )\n",
    "    \n",
    "    score_context = cs1[0][1]\n",
    "    score_title = cs2[0][1]\n",
    "    final_doc_sim_score = max(score_context, score_title)\n",
    "    \n",
    "#     doc_similarity_scores = []\n",
    "#     for i in range(len(cs1)):\n",
    "#         for j in range(len(cs1)):\n",
    "#             if j>i:\n",
    "#                 score_context = cs1[i][j]\n",
    "#                 score_title = cs2[i][j]\n",
    "#                 final_doc_sim_score = max(score_context, score_title)\n",
    "#                 doc_similarity_scores.append([News[i][0], News[j][0], final_doc_sim_score])\n",
    "    \n",
    "    return [NewsIndex2, final_doc_sim_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similar_Document_Content_News(NewsIndex1, emb_dict):\n",
    "    from tqdm import tqdm\n",
    "    doc_similarity_scores = []\n",
    "    for NewsIndex2 in tqdm(emb_dict):\n",
    "        if NewsIndex2!=NewsIndex1:\n",
    "            doc_similarity_scores.append(Calculate_News_Document_Similarity(NewsIndex1, NewsIndex2, emb_dict))\n",
    "            \n",
    "    return doc_similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5229/5229 [04:11<00:00, 20.83it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_similarity_scores = Similar_Document_Content_News(NewsID,emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
